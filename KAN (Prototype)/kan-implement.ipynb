{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":201716,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":172100,"modelId":194446}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport torch.nn as nn\n\n# ===================== KAN Layer =====================\nclass KANLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, num_intervals=10, spline_degree=3, device=\"cpu\"):\n        super(KANLayer, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.num_intervals = num_intervals\n        self.spline_degree = spline_degree\n        self.device = device\n\n        self.grid = torch.linspace(-1, 1, num_intervals + 1).repeat(input_dim, 1).to(device)\n        self.spline_coeffs = nn.Parameter(\n            torch.randn(num_intervals + spline_degree, input_dim, output_dim).to(device)\n        )\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        processed_features = []\n        for i in range(self.input_dim):\n            feature = x[:, i]\n            spline_output = self.evaluate_splines(feature, self.grid[i], self.spline_coeffs[:, i, :])\n            processed_features.append(spline_output)\n\n        preprocessed_data = torch.stack(processed_features, dim=1).sum(dim=1)\n        return self.fc(preprocessed_data)\n\n    @staticmethod\n    def evaluate_splines(x, grid, coeffs):\n        degree = coeffs.size(0) - (grid.size(0) - 1)\n        spline_output = torch.zeros(x.size(0), coeffs.size(1), device=x.device)\n        for d in range(degree):\n            spline_output += coeffs[d] * (x.unsqueeze(1) ** d)\n        return spline_output\n\n# ===================== Full KAN Model =====================\nclass FullKAN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FullKAN, self).__init__()\n        self.input_projection = nn.Linear(input_dim, hidden_dim)\n        self.layer1 = KANLayer(hidden_dim, hidden_dim)\n        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n        self.layer3 = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        x = self.input_projection(x)\n        x = torch.relu(x)\n        x = self.layer1(x)\n        x = torch.relu(x)\n        x = self.dropout(x)\n        x = self.layer2(x)\n        x = torch.relu(x)\n        return self.layer3(x)\n\n# ===================== Image Transformation =====================\ntransform = transforms.Compose([\n    transforms.Grayscale(),\n    transforms.Resize((48, 48)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# ===================== Predict Emotion =====================\ndef predict_emotion(image_path, model, device):\n    \"\"\"\n    Predict the emotion from an input image using the trained model.\n    \n    Args:\n        image_path (str): Path to the input image.\n        model (nn.Module): The trained emotion detection model.\n        device (torch.device): The device to run the model on (cpu or cuda).\n        \n    Returns:\n        str: Predicted emotion label.\n    \"\"\"\n    image = Image.open(image_path)\n    image = transform(image).unsqueeze(0).to(device)\n    \n    model.eval()\n    with torch.no_grad():\n        output = model(image.view(image.size(0), -1))\n        predicted_class = output.argmax(dim=1).item()\n\n    emotion_labels = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sadness', 'Surprise', 'Neutral']\n    return emotion_labels[predicted_class]\n\n# ===================== Example Usage =====================\nimage_path = '/kaggle/input/fer2013/train/sad/Training_10115766.jpg'  # Provide the path to your input image\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = FullKAN(input_dim=48*48, hidden_dim=256, output_dim=7).to(device)\n\n# Load the trained model weights safely\nmodel_weights_path = \"/kaggle/input/emotion-classifier-using-kan/pytorch/default/1/kan_emotion_model.pth\"\nmodel.load_state_dict(torch.load(model_weights_path, weights_only=True))\n\n# Predict and print the emotion\npredicted_emotion = predict_emotion(image_path, model, device)\nprint(f\"Predicted Emotion: {predicted_emotion}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:12:19.737477Z","iopub.execute_input":"2024-12-17T21:12:19.737986Z","iopub.status.idle":"2024-12-17T21:12:19.817018Z","shell.execute_reply.started":"2024-12-17T21:12:19.737944Z","shell.execute_reply":"2024-12-17T21:12:19.815759Z"}},"outputs":[{"name":"stdout","text":"Predicted Emotion: Anger\n","output_type":"stream"}],"execution_count":17}]}